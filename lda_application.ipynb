{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from lda_model import LDA\n",
    "from preprocessing import Preprocessing\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation : an application\n",
    "\n",
    "## Authors :\n",
    "- Mathis Demay\n",
    "- Luqman Ferdjani\n",
    "\n",
    "## Purpose of the notebook :\n",
    "\n",
    "The idea behind this notebook is to apply Latent Dirichlet Analysis, a generative model for document topic prediction. We shall first give a short explanation of what LDA is before applying it to a well known dataset found in sklearn, the 20 newsgroup dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is LDA ? A succint explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA is a model which posits that each document is generated as a mixture of topics where all proportions (words within a topic or within a document, proportions of each topic within a document) are distributed according to latent Dirichlet random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This DAG presents the model : \n",
    "    \n",
    "<img src=\"images/dag_lda.png\"\n",
    "     alt=\"DAG of lda\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With :\n",
    "\n",
    "<ul> \n",
    "    <li>N the total number of different words</li> \n",
    "    <li>M the number of documents in total</li>\n",
    "    <li>$\\alpha$ the concentrations of the Dirichlet distribution used to generate theta</li>\n",
    "    <li>$\\theta$ : a topic mixture. A vector of topic proportions within a document. Topics are distributed according to a multinomial law</li>\n",
    "    <li>z : the topic</li>\n",
    "    <li>$\\beta$ : a matrix of size k $\\times$ V where V is the total number of different words and k is the total number of different topics. Each line is indexed by a topic and each column by a word.</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "<b>Important precision</b> : LDA uses unigrams. Each document is basically treated as a bag of word where each bag is of size one word. This works because the articulation of words within a topic is not necessary to finding its topic. For example fast skimming through a biology article talking about \"dna\", \"rna\", \"genomics\" without finding how these notions are linked within the document one could assume with high probability that the article is about molecular biology.\n",
    "\n",
    "More information detailed information on what is LDA, the idea behind the model, the model itself, the applications, the inferential methods are found in the detailed report present in the repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presentation of the newsgroup dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 20,000 newsgroups about 20 different topics. Why choose this dataset ? \n",
    "\n",
    "- It is easy to find as it is loadable via the sklearn API\n",
    "- It has great documentation on how to pre-process it\n",
    "- It is a dataset comprised of documents categorized by topics, which is exactly what LDA is made for. Additionaly, with these provided topics, we can assess the quality of our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = fetch_20newsgroups(subset='train')\n",
    "test = fetch_20newsgroups(subset='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example of a newsgroup \n",
    "\n",
    "print(train.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first start by stripping the documents of headers footers and quotes. Why ? Because these are parts of newgroups which are irrelevant to topic prediction. Headers and footers typically contain information about the author, references to other newsgroups, locations, etc ... These features could cause our model to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset contains, 11314 training examples\n",
      "The testing dataset contains, 7532 training examples\n"
     ]
    }
   ],
   "source": [
    "print(\"The training dataset contains\", len(train.data), \"training examples\")\n",
    "print(\"The testing dataset contains\", len(test.data), \"test examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example of a newsgroup\n",
    "\n",
    "print(train.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also notice something else about the dataset, the presence of special characters and many words which are not relevant to topic analysis such as determinants, common verbs and words etc ... Words that pertain to many different topics and don't give any clear indication. However this will be done in the pre-processing part of our notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Target topics, these are already coded by numbers\n",
    "\n",
    "train.target[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Topics for all newgroups\n",
    "\n",
    "train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These topics can get very specific, we shall boil them down to broader topics to simplify the job for our model. We can already notice broader topics about politics, religion, technology, sports, sales, etc ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/Lucky/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "pp = Preprocessing()\n",
    "proc_corpus = pp.corpus_preproc(train)\n",
    "d, bow = pp.build_bow(proc_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "E-Step\n",
      "E-step through 0 documents\n",
      "Iteration 0 of variational parameters estimation\n",
      "-10.708642172506947\n",
      "Iteration 1 of variational parameters estimation\n",
      "-10.708642172506947\n",
      "Iteration 0 of variational parameters estimation\n",
      "-10.708642172506947\n",
      "Iteration 1 of variational parameters estimation\n",
      "-10.708642172506947\n",
      "Iteration 0 of variational parameters estimation\n",
      "-10.708642172506947\n",
      "Iteration 1 of variational parameters estimation\n",
      "-10.708642172506947\n",
      "Iteration 0 of variational parameters estimation\n",
      "-10.708642172506947\n",
      "Iteration 1 of variational parameters estimation\n",
      "-10.708642172506947\n",
      "Iteration 0 of variational parameters estimation\n",
      "-10.708642172506947\n",
      "Iteration 1 of variational parameters estimation\n",
      "-10.708642172506947\n",
      "M-Step\n",
      "0.0\n",
      "iteraton 0 -53.543210862534735\n",
      "Iteration: 1\n",
      "E-Step\n",
      "E-step through 0 documents\n",
      "Iteration 0 of variational parameters estimation\n",
      "-10.708642172506947\n",
      "Iteration 1 of variational parameters estimation\n",
      "-10.708642172506947\n",
      "Iteration 0 of variational parameters estimation\n",
      "-10.708642172506947\n",
      "Iteration 1 of variational parameters estimation\n",
      "-10.708642172506947\n",
      "Iteration 0 of variational parameters estimation\n",
      "-10.708642172506947\n",
      "Iteration 1 of variational parameters estimation\n",
      "-10.708642172506947\n",
      "Iteration 0 of variational parameters estimation\n",
      "-10.708642172506947\n",
      "Iteration 1 of variational parameters estimation\n",
      "-10.708642172506947\n",
      "Iteration 0 of variational parameters estimation\n",
      "-10.708642172506947\n",
      "Iteration 1 of variational parameters estimation\n",
      "-10.708642172506947\n",
      "M-Step\n",
      "0.0\n",
      "iteraton 1 -53.543210862534735\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]]),\n",
       " array([[1.2, 1.2, 1.2, 1.2, 1.2],\n",
       "        [1.2, 1.2, 1.2, 1.2, 1.2],\n",
       "        [1.2, 1.2, 1.2, 1.2, 1.2],\n",
       "        [1.2, 1.2, 1.2, 1.2, 1.2],\n",
       "        [1.2, 1.2, 1.2, 1.2, 1.2]]),\n",
       " array([[[0., 0., 0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0., 0., 0.]]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LDA(10, bow, d, alpha=1, set_alpha=True)\n",
    "\n",
    "lda.estimation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
